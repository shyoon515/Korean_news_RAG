{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c2aa0f",
   "metadata": {},
   "source": [
    "# RAG Chatbot Service\n",
    "\n",
    "Gradioë¥¼ ì´ìš©í•œ í•œêµ­ì–´ RAG ì±—ë´‡ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\n",
    "- RAGChainì€ ìµœì´ˆ 1íšŒë§Œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.\n",
    "- ì´í›„ë¡œëŠ” ê°ì²´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì§ˆë¬¸ì— ëŒ€í•´ ë¹ ë¥´ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.\n",
    "- ê²€ìƒ‰ ë°©ì‹(retrieval_type)ê³¼ í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜(hybrid_alpha)ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb78e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/final_project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project path to sys.path\n",
    "project_path = str(Path('.').resolve().parent)\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "import gradio as gr\n",
    "from pipeline.chain.generation import RAGChain\n",
    "from pipeline.common import setup_logger\n",
    "from pipeline.eval.utils import answer_extractor\n",
    "from typing import Optional, Tuple\n",
    "import time\n",
    "\n",
    "# Global variables for RAGChain instance and current settings\n",
    "rag_chain: Optional[RAGChain] = None\n",
    "current_config = {\n",
    "    'retrieval_type': 'hybrid',\n",
    "    'hybrid_alpha': 0.6\n",
    "}\n",
    "logger = setup_logger(\"chatbot_logger\")\n",
    "\n",
    "print(\"âœ“ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c582f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rag_chain(retrieval_type: str, hybrid_alpha: float) -> str:\n",
    "    \"\"\"\n",
    "    RAGChain ì´ˆê¸°í™” í•¨ìˆ˜\n",
    "    - ì²˜ìŒ í˜¸ì¶œ ì‹œ RAGChainì„ ìƒì„±í•˜ê³  ì „ì—­ ë³€ìˆ˜ì— ì €ì¥\n",
    "    - ì´í›„ í˜¸ì¶œ ì‹œ ì˜µì…˜ì´ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ì¬ì´ˆê¸°í™”\n",
    "    \n",
    "    Args:\n",
    "        retrieval_type: 'sparse', 'dense', 'hybrid' ì¤‘ í•˜ë‚˜\n",
    "        hybrid_alpha: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì˜ BM25 ê°€ì¤‘ì¹˜ (0.0~1.0)\n",
    "    \n",
    "    Returns:\n",
    "        ì´ˆê¸°í™” ìƒíƒœ ë©”ì‹œì§€\n",
    "    \"\"\"\n",
    "    global rag_chain, current_config, logger\n",
    "\n",
    "    # ì„¤ì • ì—…ë°ì´íŠ¸\n",
    "    current_config['retrieval_type'] = retrieval_type\n",
    "    current_config['hybrid_alpha'] = hybrid_alpha\n",
    "\n",
    "    # ì„¤ì •ì´ ë°”ë€Œì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°ì²´ ë‚´ í•´ë‹¹ ë³€ìˆ˜ë“¤ì„ ì—…ë°ì´íŠ¸\n",
    "    if rag_chain is not None:\n",
    "        rag_chain.retrieval_type = retrieval_type\n",
    "        rag_chain.hybrid_alpha = hybrid_alpha\n",
    "\n",
    "        # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¬ì‚¬ìš©\n",
    "        msg = f\"âœ“ ê¸°ì¡´ RAGChain ì¬ì‚¬ìš©\\n- ê²€ìƒ‰ ë°©ì‹: {retrieval_type}\\n- í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜: {hybrid_alpha}\"\n",
    "        logger.info(msg)\n",
    "        return msg \n",
    "    \n",
    "    try:\n",
    "        msg = f\"ğŸ”„ RAGChain ì´ˆê¸°í™” ì¤‘...\\n- ê²€ìƒ‰ ë°©ì‹: {retrieval_type}\\n- í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜: {hybrid_alpha}\"\n",
    "        logger.info(msg)\n",
    "        print(msg)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # RAGChain ì´ˆê¸°í™” (ì‹œê°„ì´ ê±¸ë¦¼)\n",
    "        rag_chain = RAGChain(\n",
    "            retrieval_type=retrieval_type,\n",
    "            hybrid_alpha=hybrid_alpha,\n",
    "            encoder_name='bge',\n",
    "            chunk_size=1000,\n",
    "            overlap_size=200,\n",
    "            generator_name='midm',\n",
    "            seq_type='dq',\n",
    "            doc_rearrange=1,\n",
    "            top_k=5,\n",
    "            generator_type=\"vllm\",\n",
    "            vllm_api_base=\"http://localhost:8000/v1\",\n",
    "            with_retrieval_results=True,\n",
    "            logger=logger\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        success_msg = f\"âœ“ RAGChain ì´ˆê¸°í™” ì™„ë£Œ ({elapsed_time:.1f}ì´ˆ)\\n- ê²€ìƒ‰ ë°©ì‹: {retrieval_type}\\n- í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜: {hybrid_alpha}\"\n",
    "        logger.info(success_msg)\n",
    "        return success_msg\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"âœ— RAGChain ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7e2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, retrieval_type: str, hybrid_alpha: float) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± í•¨ìˆ˜\n",
    "    - RAGChainì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì´ˆê¸°í™”\n",
    "    - RAGChainì˜ .ask() ë©”ì„œë“œë¡œ ë‹µë³€ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        question: ì‚¬ìš©ì ì§ˆë¬¸\n",
    "        retrieval_type: ê²€ìƒ‰ ë°©ì‹\n",
    "        hybrid_alpha: í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜\n",
    "    \n",
    "    Returns:\n",
    "        (ë‹µë³€, ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´)\n",
    "    \"\"\"\n",
    "    global rag_chain, logger\n",
    "    \n",
    "    if not question.strip():\n",
    "        return \"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\", \"\"\n",
    "    \n",
    "    try:\n",
    "        # RAGChain ì´ˆê¸°í™” (í•„ìš”í•œ ê²½ìš°ë§Œ)\n",
    "        init_msg = initialize_rag_chain(retrieval_type, hybrid_alpha)\n",
    "        \n",
    "        if rag_chain is None:\n",
    "            return \"RAGChain ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\", \"\"\n",
    "        \n",
    "        logger.info(f\"ì§ˆë¬¸: {question}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # RAGChainì˜ .ask() ë©”ì„œë“œ í˜¸ì¶œ (ë¹ ë¦„: 1-2ì´ˆ)\n",
    "        result = rag_chain.ask(question)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # ê²°ê³¼ ì²˜ë¦¬\n",
    "        if isinstance(result, list) and len(result) > 0:\n",
    "            answer, retrieved_docs = result[0]\n",
    "            \n",
    "            # answer_extractorë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
    "            extracted_answer = answer_extractor(answer)\n",
    "            \n",
    "            # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ í¬ë§·íŒ… (ë¬¸ì„œ ë‚´ìš©ë§Œ)\n",
    "            doc_info = f\"**ê²€ìƒ‰ëœ ìƒìœ„ ë¬¸ì„œ**:\\n\\n\"\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                doc_info += f\"**{i}.** {doc['chunked_text']}...\\n\\n\"\n",
    "            \n",
    "            logger.info(f\"ë‹µë³€ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)\")\n",
    "            return extracted_answer, doc_info\n",
    "        else:\n",
    "            return \"ë‹µë³€ ìƒì„± ì‹¤íŒ¨\", \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return error_msg, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd02bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_487983/2236825600.py:5: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(title=\"RAG ì±—ë´‡ ì„œë¹„ìŠ¤\", theme=gr.themes.Soft()) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def create_gradio_interface():\n",
    "    \"\"\"\n",
    "    Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ë°˜í™˜\n",
    "    \"\"\"\n",
    "    with gr.Blocks(title=\"RAG ì±—ë´‡ ì„œë¹„ìŠ¤\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"# ğŸ¤– RAG ê¸°ë°˜ í•œêµ­ì–´ ì±—ë´‡ ì„œë¹„ìŠ¤\")\n",
    "        gr.Markdown(\"ë‰´ìŠ¤ ê¸°ì‚¬ ê¸°ë°˜ ê²€ìƒ‰ì¦ê°•ìƒì„±(RAG) ì±—ë´‡ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œ í›„ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                question_input = gr.Textbox(\n",
    "                    label=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”\",\n",
    "                    placeholder=\"ì˜ˆ: ìµœê·¼ ì •ì¹˜ ë‰´ìŠ¤ëŠ” ì–´ë–»ê²Œ ë¼?\",\n",
    "                    lines=3\n",
    "                )\n",
    "                ask_button = gr.Button(\"ğŸ” ë‹µë³€ ìƒì„±\", variant=\"primary\", size=\"lg\")\n",
    "            \n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### âš™ï¸ ê²€ìƒ‰ ì˜µì…˜\")\n",
    "                retrieval_type_select = gr.Radio(\n",
    "                    choices=[\"sparse\", \"dense\", \"hybrid\"],\n",
    "                    value=\"hybrid\",\n",
    "                    label=\"ê²€ìƒ‰ ë°©ì‹\",\n",
    "                    info=\"BM25(sparse) / ë²¡í„° ê²€ìƒ‰(dense) / í˜¼í•©(hybrid)\"\n",
    "                )\n",
    "                hybrid_alpha_slider = gr.Slider(\n",
    "                    minimum=0.0,\n",
    "                    maximum=1.0,\n",
    "                    value=0.6,\n",
    "                    step=0.1,\n",
    "                    label=\"í˜¼í•© ê²€ìƒ‰ ê°€ì¤‘ì¹˜\",\n",
    "                    info=\"0(BM25ë§Œ) ~ 1(ë²¡í„° ê²€ìƒ‰)\",\n",
    "                    visible=True\n",
    "                )\n",
    "        \n",
    "        # ì˜µì…˜ ê°€ì‹œì„± ì œì–´\n",
    "        def update_alpha_visibility(retrieval_type):\n",
    "            return gr.update(visible=(retrieval_type == \"hybrid\"))\n",
    "        \n",
    "        retrieval_type_select.change(\n",
    "            update_alpha_visibility,\n",
    "            inputs=retrieval_type_select,\n",
    "            outputs=hybrid_alpha_slider\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                answer_output = gr.Markdown(\n",
    "                    label=\"ë‹µë³€\",\n",
    "                    value=\"ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ì—¬ê¸°ì— ë‹µë³€ì´ í‘œì‹œë©ë‹ˆë‹¤.\"\n",
    "                )\n",
    "            with gr.Column():\n",
    "                docs_output = gr.Markdown(\n",
    "                    label=\"ê²€ìƒ‰ëœ ë¬¸ì„œ\",\n",
    "                    value=\"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.\"\n",
    "                )\n",
    "        \n",
    "        with gr.Row():\n",
    "            status_output = gr.Textbox(\n",
    "                label=\"ìƒíƒœ\",\n",
    "                interactive=False,\n",
    "                value=\"ì¤€ë¹„ ì™„ë£Œ\"\n",
    "            )\n",
    "        \n",
    "        # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸\n",
    "        def on_ask_click(question, retrieval_type, hybrid_alpha):\n",
    "            status_output.value = \"ì²˜ë¦¬ ì¤‘...\"\n",
    "            \n",
    "            # retrieval_typeì´ hybridê°€ ì•„ë‹ˆë©´ hybrid_alphaëŠ” ë¬´ì‹œ\n",
    "            if retrieval_type != \"hybrid\":\n",
    "                hybrid_alpha = 0.6  # default value\n",
    "            \n",
    "            answer, docs = answer_question(question, retrieval_type, hybrid_alpha)\n",
    "            \n",
    "            status_output.value = \"ì™„ë£Œ\"\n",
    "            return answer, docs, status_output.value\n",
    "        \n",
    "        ask_button.click(\n",
    "            on_ask_click,\n",
    "            inputs=[question_input, retrieval_type_select, hybrid_alpha_slider],\n",
    "            outputs=[answer_output, docs_output, status_output]\n",
    "        )\n",
    "        \n",
    "        # Enter í‚¤ë¡œë„ ì§ˆë¬¸ ì œì¶œ ê°€ëŠ¥\n",
    "        question_input.submit(\n",
    "            on_ask_click,\n",
    "            inputs=[question_input, retrieval_type_select, hybrid_alpha_slider],\n",
    "            outputs=[answer_output, docs_output, status_output]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "demo = create_gradio_interface()\n",
    "\n",
    "print(\"âœ“ Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed04243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ RAG ì±—ë´‡ ì„œë¹„ìŠ¤ ì‹œì‘\n",
      "============================================================\n",
      "\n",
      "ğŸ“ ì‚¬ìš© ë°©ë²•:\n",
      "1. ê²€ìƒ‰ ì˜µì…˜ì„ ì„ íƒí•©ë‹ˆë‹¤ (ê¸°ë³¸ê°’: hybrid)\n",
      "2. ì§ˆë¬¸ì„ ì…ë ¥í•©ë‹ˆë‹¤\n",
      "3. 'ë‹µë³€ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ê±°ë‚˜ Enterë¥¼ ëˆ„ë¦…ë‹ˆë‹¤\n",
      "\n",
      "âš ï¸  ì£¼ì˜ì‚¬í•­:\n",
      "- ìµœì´ˆ ì‹¤í–‰ ì‹œ RAGChain ì´ˆê¸°í™”ì— ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤\n",
      "- ì´í›„ ì§ˆë¬¸ì€ ë¹ ë¥´ê²Œ ì‘ë‹µë©ë‹ˆë‹¤\n",
      "\n",
      "============================================================\n",
      "\n",
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://f3401fcc69ab60089b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f3401fcc69ab60089b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ RAGChain ì´ˆê¸°í™” ì¤‘...\n",
      "- ê²€ìƒ‰ ë°©ì‹: hybrid\n",
      "- í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-21 05:40:55] INFO SentenceTransformer.py:219: Use pytorch device_name: cuda:0\n",
      "[2025-12-21 05:40:55] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: dragonkue/bge-m3-ko\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.68it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 23.71it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 51.69it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 50.49it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 57.27it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.22it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 58.27it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 48.62it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 24.74it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 51.34it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.73it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.95it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 51.41it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 47.61it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.99it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.78it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 56.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# ì„œë¹„ìŠ¤ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸš€ RAG ì±—ë´‡ ì„œë¹„ìŠ¤ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nğŸ“ ì‚¬ìš© ë°©ë²•:\")\n",
    "    print(\"1. ê²€ìƒ‰ ì˜µì…˜ì„ ì„ íƒí•©ë‹ˆë‹¤ (ê¸°ë³¸ê°’: hybrid)\")\n",
    "    print(\"2. ì§ˆë¬¸ì„ ì…ë ¥í•©ë‹ˆë‹¤\")\n",
    "    print(\"3. 'ë‹µë³€ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ê±°ë‚˜ Enterë¥¼ ëˆ„ë¦…ë‹ˆë‹¤\")\n",
    "    print(\"\\nâš ï¸  ì£¼ì˜ì‚¬í•­:\")\n",
    "    print(\"- ìµœì´ˆ ì‹¤í–‰ ì‹œ RAGChain ì´ˆê¸°í™”ì— ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤\")\n",
    "    print(\"- ì´í›„ ì§ˆë¬¸ì€ ë¹ ë¥´ê²Œ ì‘ë‹µë©ë‹ˆë‹¤\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f50683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
